\chapter{Introduction}
The ongoing rapid growth of the available amount of data drives the urge for automated processing of such data.
\emph{Data mining} -- the means of finding new patterns in \emph{datasets} -- is now widely used in medicine, economics, bioinformatics and other important areas of human interest. Many different algorithms exist and are used for this task of pattern extraction. However, even for an expert, it is hard to choose the most suitable algorithm for a particular dataset. 
According to the \emph{No Free Lunch} theorem (NFL) stated in \cite{NoFreeLunchTheorem}, the average performances of data mining algorithms on all data mining problems are equal. It means that elevated performance of any algorithm over one class of problems is paid for in performance over another class. However, NFL considers all possible problems. We are mostly interested in so called \emph{real world} problems. Therefore, the key to success when dealing with a data mining real world problem is in binding the problem with an algorithm having elevated performance on the class of the problem. Since many fields depend on data mining techniques, it is crucial to propose and improve such bindings.

\emph{Metalearning} \cite{BrazdilMetalearning-2009} -- the learning how to learn -- can tackle the issue. 
The main idea behind metalearning is that \emph{machine learning} methods are supposed to perform similarly on similar datasets. Therefore, the notion of dataset similarity is crucial. In most cases, the similarity is computed using data characterizing the datasets -- the \emph{metadata}. Metalearning techniques use the metadata and previous experience to predict the performance of machine learning methods on new datasets. In essence, metalearning does not differ much from the traditional machine learning. The main difference is that metalearning works with the metadata of given datasets instead of the actual data, and that the result of metalearning is a recommendation of a machine learning method to use. 

The metadata may contain general information about the dataset, like the number of instances and attributes, the number of classes, performance of some algorithm over the dataset, etc. There is, however, a fixed amount of such general information extracted out of each dataset.  Many traditional methods compare datasets based on such measures.

The attribute-specific metadata include more fine-grained information about the dataset. On the other hand, each dataset can have a different number of attributes (and consequently, different amount of attribute-specific metadata), which leads to a non-propositional representation of the datasets using attribute-specific metadata. The problem of defining the similarity on the non-propositional space is non-trivial as stated by Kalousis in \cite{RepresentationalIssuesInMetalearning}. The authors of \cite{BrazdilMetalearning-2009} also explicitly recognize the problem of handling datasets described by the varying amount of metafeatures.
In the past there were only few attempts to define the non-propositional similarity. Furthermore, most of these attempts loose important information in the process.

One of the goal of this thesis is to propose algorithms capable of handling such a non-propositional representation. We take into consideration other methods dealing with non-propositional data, either directly in the metalearning field or in other fields as well.
Another goal of this thesis is to investigate theoretical properties of proposed distance measures and compare them with the related state of the art methods. We investigate whether the methods satisfy \emph{metric} axioms, alternatively, what properties must be satisfied in order to do so.

The performance of the proposed methods should be evaluated on data. Several machine learning repositories are reviewed. The emphasis is put on the amount of data provided, types of metadata available, and the fact whether the data are publicly accessible allowing for the independent re-evaluation of our results.


\section{Outline of the Thesis}
The structure of the thesis is as follows. Chapter \ref{chapter:preliminaries} introduces the field of metalearning. Different types of scenarios addressed by metalearning are presented. The literature that covers different ideas and advances in metalearning is reviewed. The algorithm ranking based on distance measures is addressed in more depth. Unified workflow for measuring the quality of distance based algorithm ranking is presented. We also cover some of our contributions related to metalearning that are not directly related to the main goals of the thesis. This includes our recommendation multi-agent system \emph{Pikater} and a \emph{hierarchical clustering} approach for metalearning.  

Chapter \ref{chapter:globalDistance} reviews the metric spaces and several well known facts about metrics spaces. We look into the commonly used dataset distance measures based on the fixed amount of metadata extracted from datasets and what properties are necessary in order to get a metric.

The main contribution of the thesis is presented in Chapter \ref{chapter:attributeAssignment}. First, we discuss the motivation behind dealing with objects with variable structure and review some work not directly related to metalearning, which however served as an inspiration for the work in this thesis. We also discuss why we cannot use such techniques to address the variable amount of attributes when comparing the datasets. We review state of the art approaches to handle the attribute-specific metadata in metalearning and discuss their strengths and weaknesses. The idea of attribute assignment is presented. The main idea lies in defining attribute distance measure and aligning the attributes as best as we can. The desired distance between datasets is the sum of distances between aligned attributes. Several algorithms based on this idea are proposed. We also show what properties we have to maintain in order to get the metric on the dataset space. We also discuss other ideas we considered, which could, however, violate some of metric axioms.

Chapter \ref{chapter:obtainingData} discusses data needed to conduct our experiments. Several databases of machine learning datasets are reviewed including those containing results of machine learning algorithms over those datasets. We discuss the metadata available about the datasets and establish attribute-specific metadata that we will extract ourselves in order to be able to build attribute assignment models.

The first batch of experiments is proposed in Chapter \ref{chapter:metricExperiments}. All the pieces and algorithms are glued together and the workflow is reviewed as a whole. Experiments are proposed in such a way that we always get a metric on the dataset level. We let the \emph{Genetic Algorithms} \cite{HollandGeneticAlgorithms} optimize weights of the attribute assignment techniques. We also review the complexity of the whole workflow that will be important when proposing the experiment settings. Our experiments will be actually solving \emph{reinforcement learning} \cite{aima3ed} task with very rare feedback from the environment. The results of experiments are discussed and compared between themselves and a \emph{baseline} algorithm. 

Chapter \ref{chapter:metricRelaxation} introduces more expressive language to describe attribute distance measure while relaxing the requirement on dataset metric. We will show that the relaxation will guarantee a semimetric on the attribute space. We discuss whether the missing triangle inequality is necessarily important for the distance measure. We will also prove that the semimetric on the attribute level lead to a semimetric on the dataset level using our attribute assignment workflow. We conduct another batch of experiments with the new algorithms, and discuss their results.

In Chapter \ref{chapter:regularization}, we discuss set of techniques that could improve the generalization abilities of some of our models. In particular, we discuss coevolution, bloat control and bootstrapping  of the population as an extension of the genetic algorithms. We also discuss using \emph{multi-objectivization} to split the objective function into two. The second -- added -- criterion is the metric similarity. We also define the \emph{multi-objective optimization} and describe one algorithm in particular -- \emph{NSGA-II} \cite{nsgaII}. We conduct experiments with coevolution, bootstrapping and bloat control and compare their results with the previous experiments. We review one of our previous works  with multi-objectivization.

Chapter \ref{chapter:conclusion} concludes the thesis. The goals of the thesis are evaluated based on the expectations. The future work and possible opportunities to improve the results are outlined.

 

 